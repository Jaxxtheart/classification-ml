{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hult International Business School \n",
    "Assignment: Individual Classification Assignment \n",
    "Student : Mosiuwa Tshabalala \n",
    "Subject : Machine Learning ðŸ˜Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the relevant packages for data science essentials, modeling and tuning\n",
    "import pandas as pd\n",
    "import numpy as np    \n",
    "from sklearn.model_selection import RandomizedSearchCV                   \n",
    "from sklearn.model_selection import train_test_split                   \n",
    "from sklearn.model_selection import GridSearchCV             \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler     \n",
    "from sklearn.metrics import confusion_matrix        \n",
    "from sklearn.metrics import roc_auc_score          \n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# setting pandas print options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "file = 'Apprentice_Chef_Dataset.xlsx'\n",
    "model_performance = pd.read_excel('./model_results/classification_model_performance.xlsx')\n",
    "\n",
    "ac_dataset = pd.read_excel(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary to store candidate models\n",
    "\n",
    "ac_logdict = {\n",
    "\n",
    " # full model\n",
    " 'ac_logfull'   : ['REVENUE', 'TOTAL_MEALS_ORDERED', 'UNIQUE_MEALS_PURCH','CONTACTS_W_CUSTOMER_SERVICE', \n",
    "                     'PRODUCT_CATEGORIES_VIEWED', 'AVG_TIME_PER_SITE_VISIT', 'MOBILE_NUMBER', 'CANCELLATIONS_BEFORE_NOON', \n",
    "                     'CANCELLATIONS_AFTER_NOON','TASTES_AND_PREFERENCES', 'PC_LOGINS', \n",
    "                     'MOBILE_LOGINS', 'WEEKLY_PLAN', 'EARLY_DELIVERIES','LATE_DELIVERIES', \n",
    "                     'PACKAGE_LOCKER', 'REFRIGERATED_LOCKER', 'AVG_PREP_VID_TIME', \n",
    "                     'LARGEST_ORDER_SIZE', 'MASTER_CLASSES_ATTENDED',  'MEDIAN_MEAL_RATING', \n",
    "                     'AVG_CLICKS_PER_VISIT', 'TOTAL_PHOTOS_VIEWED']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_datadrop = ac_dataset.drop(['NAME','EMAIL', 'FIRST_NAME', 'FAMILY_NAME'], axis = 1)\n",
    "ac_data   =  ac_datadrop.loc[ : , ac_logdict['ac_logfull']]\n",
    "ac_target =  ac_dataset.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "#  Processing the  model creating the scaled dataframe\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(ac_data)\n",
    "\n",
    "x_scaled = scaler.transform(ac_data)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled,\n",
    "                                                    ac_target, \n",
    "                                                    test_size = 0.25,\n",
    "                                                    random_state = 219,\n",
    "                                                    stratify = ac_target\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JacobT\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:278: UserWarning: The total space of parameters 1 is smaller than n_iter=1000. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# creating a hyperparameter grid\n",
    "param_grid = {'max_depth'        : [8],\n",
    "              'min_samples_leaf' : [1],\n",
    "              'splitter'         : ['random'],\n",
    "              'criterion'        : ['gini']}\n",
    "\n",
    "\n",
    "# Instantiating the model object using best hyperparameters\n",
    "\n",
    "ac_dtmodel = DecisionTreeClassifier(random_state = 219)\n",
    "\n",
    "\n",
    "# RandomizedSearchCV object\n",
    "model_tuned = RandomizedSearchCV( estimator         = ac_dtmodel,\n",
    "                                param_distributions = param_grid,\n",
    "                                cv                  = 3,\n",
    "                                n_iter              = 1000,\n",
    "                                random_state        = 219,\n",
    "                                )\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "model_fin = model_tuned.fit(x_scaled, ac_target)\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "model_fit_pred = (model_fin.predict_proba(x_test)[:,1]>=0.59).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_dt_train_acc = model_fin.score(x_train, y_train).round(4)\n",
    "tuned_dt_test_acc  = model_fin.score(x_test, y_test).round(4)\n",
    "tuned_dt_auc       = roc_auc_score(y_true  = y_test,\n",
    "                                   y_score = model_fit_pred).round(4)\n",
    "dt_tn, \\\n",
    "dt_fp, \\\n",
    "dt_fn, \\\n",
    "dt_tp = confusion_matrix(y_true = y_test, y_pred = model_fit_pred).ravel()\n",
    "\n",
    "# creating a dictionary for model results\n",
    "model_performance = {\n",
    "    \n",
    "    'Model Name'    : ['Decision Tree'],\n",
    "           \n",
    "    'AUC Score' : [tuned_dt_auc],\n",
    "    \n",
    "    'Training Accuracy' : [tuned_dt_train_acc],\n",
    "           \n",
    "    'Testing Accuracy'  : [tuned_dt_test_acc],\n",
    "\n",
    "    'Confusion Matrix'  : [(dt_tn, dt_fp, dt_fn,dt_tp)]}\n",
    "\n",
    "# converting model_performance into a DataFrame\n",
    "model_performance_df = pd.DataFrame(model_performance)\n",
    "\n",
    "\n",
    "# sending model results to Excel\n",
    "model_performance_df.to_excel('./model_results/classification_model_performance.xlsx',\n",
    "                           index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a hyperparameter grid\n",
    "param_grid = {\n",
    "    'bootstrap': [False],\n",
    "    'criterion': ['entropy'],\n",
    "    'max_depth': [0,8],\n",
    "    'max_features' : ['auto'],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12,20],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "    }\n",
    "\n",
    "# instantiating the model object using best parameters\n",
    "ac_rfmodel = RandomForestClassifier()\n",
    "\n",
    "ac_grid = GridSearchCV(estimator = ac_rfmodel, \n",
    "                       param_grid = param_grid, \n",
    "                       cv = 3,\n",
    "                       n_jobs = -1)\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "ac_grid_fit = ac_grid.fit(x_scaled, ac_target)\n",
    "\n",
    "\n",
    "# PREDICT step is not needed\n",
    "model_pred = (ac_grid_fit.predict_proba(x_test)[:,1]>=0.59).astype(int)\n",
    "\n",
    "# declaring model performance objects\n",
    "tuned_rf_train_acc = ac_grid_fit.score(x_train, y_train).round(4)\n",
    "tuned_rf_test_acc  = ac_grid_fit.score(x_test, y_test).round(4)\n",
    "tuned_rf_auc       = roc_auc_score(y_true  = y_test,\n",
    "                                   y_score = model_pred).round(4)\n",
    "rf_tn, \\\n",
    "rf_fp, \\\n",
    "rf_fn, \\\n",
    "rf_tp = confusion_matrix(y_true = y_test, y_pred = model_pred).ravel()\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance_df.append(\n",
    "                          {'Model Name'         : 'Random Forest',\n",
    "                           'Training Accuracy'  : tuned_rf_train_acc,\n",
    "                           'Testing Accuracy'   : tuned_rf_test_acc,\n",
    "                           'AUC Score'          : tuned_rf_auc,\n",
    "                           'Confusion Matrix'   : (rf_tn,\n",
    "                                                   rf_fp,\n",
    "                                                   rf_fn,\n",
    "                                                   rf_tp)\n",
    "                          },\n",
    "                          ignore_index = True)\n",
    "# sending model results to Excel\n",
    "model_performance_df.to_excel('./model_results/classification_model_performance.xlsx',\n",
    "                           index = False)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the model performance I have decided to choose the Random Forest Model as my most \n",
      "optimal model. This model shows the best balance between specifity and recall & the difference between \n",
      "false negatives and true negatives is large showing that our model classifications are not left to chance. \n",
      "Although the Decision Tree classified model has larger training and testing scores the Random forest \n",
      "has the best distribution of the Confusion Matrix and hence a higher AUC Score\n",
      "FINAL MODEL: RANDOM FOREST\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_performance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e07d3c6cece3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbest\u001b[0m \u001b[0mdistribution\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mConfusion\u001b[0m \u001b[0mMatrix\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhence\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhigher\u001b[0m \u001b[0mAUC\u001b[0m \u001b[0mScore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m FINAL MODEL: RANDOM FOREST''') \n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmodel_performance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model_performance' is not defined"
     ]
    }
   ],
   "source": [
    "# checking the results\n",
    "print(f'''Based on the model performance I have decided to choose the Random Forest Model as my most \n",
    "optimal model. This model shows the best balance between specifity and recall & the difference between \n",
    "false negatives and true negatives is large showing that our model classifications are not left to chance. \n",
    "Although the Decision Tree classified model has larger training and testing scores the Random forest \n",
    "has the best distribution of the Confusion Matrix and hence a higher AUC Score\n",
    "FINAL MODEL: RANDOM FOREST''') \n",
    "model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
